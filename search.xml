<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>3月15日组会</title>
      <link href="/2023/03/15/3-15-zu-hui/"/>
      <url>/2023/03/15/3-15-zu-hui/</url>
      
        <content type="html"><![CDATA[<p>[toc]<br>#3/15日论文预读</p><h2 id="MIL"><a href="#MIL" class="headerlink" title="MIL"></a>MIL</h2><p>##<strong>多实例学习</strong></p><p>####<strong>数据格式：</strong></p><p>数据的基本单位是<strong>bag</strong>。数据的<strong>positive</strong>与<strong>negative</strong>定义如下：<br>一个bag中包含多个instance，如果<strong>所有</strong>的instance都被标记为negative，那么这个包就是negative，反之这个包为positive。<br><strong>公式表示如下：</strong><br><img src="https://gitee.com/half-lang/img/raw/master/202303141632218.png"><br>当一个<strong>bag</strong>中左右的例子都为负例时该包被称作负包，反之为正包</p><p>####问题应用场景：</p><p>多实例学习的应用与弱监督相似，具体为假设每一个<strong>bag</strong>为一组密码，对于一个<strong>任务task</strong>，有一个密码密码是能够解密的。那么训练的目的是为了分辨出哪组密码中含有可以解密的密码并指出哪条密码可以解密。<br>又比如某网站希望建立一组防火墙以屏蔽某些敏感词汇，但将所有敏感词汇标为负样本工作量过大，于是程序员使用孙笑川吧的发言作为负包（<strong>negative bag</strong>）在此假设孙吧属鼠的所有发言都是敏感词汇，同时使用语文课本上的句子（<strong>positive bag</strong>）作为正包进行训练。但值得注意的是课本上并不是所有的样本都是正样本，如鲁智深拳打镇关西中：鲁达：呸！俺只道那个郑大官人，却原来是杀猪的郑屠，这个<strong>腌臜泼才！</strong></p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://gitee.com/half-lang/img/raw/master/202303141653078.png">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">鲁智深拳打镇关西</div></center><p>####问题求解方法</p><p>假如我们已经知道所有的样本标注，这将转换为一个全监督问题。但实施情况是对于<strong>negative bag</strong> 显然所有的样本都是负样本，同时对于<strong>positive bag</strong> 我们不知道所有样本的标签。<br>解决这个问题的方法其实挺直接的：**迭代优化（alternative optimization)**。也就是说，我们先假设已经知道了所有样本的标记，那么就可以通过某种监督学习的方法得到一个分类模型，通过这个模型我们可以对每个训练样本进行预测，然后更新它们的标记，我们又可以拿这一次新得到的标记重新训练分类模型了。所以整个优化过程分为两部分：监督学习，标记更新。</p><p>多实例学习模型的基本流程：</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://gitee.com/half-lang/img/raw/master/202303141723131.png">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">多实例学习的流程</div></center>1.将实例转换为低维嵌入（特征提取）<p>2.通过置换不变聚合函数传递嵌入<br>3.预测包类别的概率</p><p>另一篇博客中也有多实例学习的描述：<br><a href="https://cqu-student.github.io/2022/12/15/2022-12-15-12-yue-17-ri-zu-hui/">https://cqu-student.github.io/2022/12/15/2022-12-15-12-yue-17-ri-zu-hui/</a></p><p>参考：<br><a href="https://zhuanlan.zhihu.com/p/377220948">https://zhuanlan.zhihu.com/p/377220948</a><br><a href="https://zhuanlan.zhihu.com/p/299819082">https://zhuanlan.zhihu.com/p/299819082</a><br><a href="https://wenku.baidu.com/view/a66fab43f12d2af90242e6da.html">https://wenku.baidu.com/view/a66fab43f12d2af90242e6da.html</a> （周志华 多实例学习）<br>##多任务学习<br>##Multi-task Learning</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://gitee.com/half-lang/img/raw/master/202303142005326.png">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">单任务学习vs多任务学习</div></center>在多任务学习中通过同一个模型可以完成多个维度的信息。###多任务学习的基本模型框架通常将多任务学习方法分为：**hard parameter sharing**和**soft parameter sharing**。区别在于上图中**MTL**模块<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://gitee.com/half-lang/img/raw/master/202303142012729.png">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">硬共享与软共享</div></center>###hard parameter sharing：在硬共享中不论多少个任务，模型底层的参数是统一的，而顶层的参数独立。由于对于所有的任务，部分参数是共享的，硬共享的方式降低了模型过拟合的概率。同时，共享的参数越多过拟合的概率越小。###soft parameter sharing：软共享方式是现代研究中重点研究的方向。在软共享模型中，模型底层的部分参数共享，同时还有自己独特的参数，顶层参数独享。<center>     <img style="border-radius: 0.3125em;     box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://gitee.com/half-lang/img/raw/master/202303142019726.png">     <br>     <div style="color:orange; border-bottom: 1px solid #d9d9d9;     display: inline-block;     color: #999;     padding: 2px;">谷歌MMOE模型</div> </center>在上述模型图中最左边模型属于硬共享，（b）与（c）都采用Expert0、Expert1、Expert3三个网络的参数求和后送入TowerA&amp;B，不同的是引入了Gate进行加权求和来表示不同任务的不同参数。<p>####多任务模型的share bottom<br>多任务学习的常规思路是共享底部的抽象表示层。<br>图像识别的底层特征往往代表一些像素纹理之类的抽象特征，而跟具体的任务不是特别相关，因此这种低冲突的表示层共享是比较容易出效果的，并且可以减少多任务的计算量（如上图a中所示）。</p><center>     <img style="border-radius: 0.3125em;     box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://gitee.com/half-lang/img/raw/master/202303142033947.png">     <br>     <div style="color:orange; border-bottom: 1px solid #d9d9d9;     display: inline-block;     color: #999;     padding: 2px;">猫狗图像分类任务</div> </center>由于上述两个模型的底层网络十分相似，于是当两个任务使用相同的抽象表示层时，同时学好两个网络是可行的。但是对于相似度较差的任务，比如识别轮胎与猫，这种模型就较为困难了。**MMoE**模型引入了新的共享机制，即将模型的共享曾划分为多个**Expert**，并使用权重参数**gate**机制使得不同任务可以个性化的使用Expert。<center>     <img style="border-radius: 0.3125em;     box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://gitee.com/half-lang/img/raw/master/202303142039485.png">     <br>     <div style="color:orange; border-bottom: 1px solid #d9d9d9;     display: inline-block;     color: #999;     padding: 2px;">Share Bottom to MoE</div> </center><p>参考资料：<br><a href="https://zhuanlan.zhihu.com/p/348873723">https://zhuanlan.zhihu.com/p/348873723</a><br><a href="https://www.jianshu.com/p/0f3e40bfd3ce">https://www.jianshu.com/p/0f3e40bfd3ce</a><br>##多标签学习（Multi-label Learning）<br>一般来说分类任务中每个对象只属于一个类别，类别之间是相互排斥的。但是在实际应用中，一个对象可能同时属于多个类别，比如一部电影可以同时是爱情片与战争片。<br><strong>多标签是一般分类问题的扩展</strong><br>###多标签分类方法<br>接下来，我将粗略的介绍现有的分类方法。主流文章将现有的多标签分类方法分为两大类：1)问题转换法 <strong>problem transformation</strong> 2) 适应法 <strong>algorithm adaptation</strong>。<br>问题转换法，顾名思义，就是将问题进行转换，它将多标签分类问题转化为多个简单的单标签分类问题。<br>适应法则是修改现有的单标签分类算法以适应解决多标签分类问题。比如修改后决策树 : 最后每一个叶子都将对应一个标签组合；修改SVM或者神经网络等，对每个标签得到的值进行排序，然后通过最小化ranking loss进行学习，等等。<br>参考资料：<br><a href="https://zhuanlan.zhihu.com/p/386597398">https://zhuanlan.zhihu.com/p/386597398</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文理论预读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文理论预读 </tag>
            
            <tag> 矮半截 </tag>
            
            <tag> 多实例学习 </tag>
            
            <tag> 多任务学习 </tag>
            
            <tag> 3月15日组会 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2023/01/14/xiao-nang-nei-jing-wen-xian-yue-du-bi-ji/"/>
      <url>/2023/01/14/xiao-nang-nei-jing-wen-xian-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="胶囊内镜文献阅读笔记"><a href="#胶囊内镜文献阅读笔记" class="headerlink" title="胶囊内镜文献阅读笔记"></a>胶囊内镜文献阅读笔记</h1><p><img src="https://gitee.com/half-lang/img/raw/master/202301132225291.png" alt="image-20230113222526251"></p><p><img src="https://gitee.com/half-lang/img/raw/master/202301132224123.png" alt="image-20230113222420980"></p><p>该论文的过程分为三个部分：</p><p>1.使用封层聚类将图片集分为不同的类</p><p>2.提取显著性特征</p><p>3.使用特征对图片进行阈值化处理提取关键帧</p><p><img src="https://gitee.com/half-lang/img/raw/master/202301192209376.png" alt="image-20230119220944196"></p><h3 id="3-1-基于密集特征的FSL的公式化"><a href="#3-1-基于密集特征的FSL的公式化" class="headerlink" title="3.1.基于密集特征的FSL的公式化"></a>3.1.基于密集特征的FSL的公式化</h3><p>​在N-way K-shot集中，目标是预测给定N个支持类的单个查询图像的类标签，每个支持类包含K个支持图像。</p><p>​输入图像编码为r个二维密集特征<img src="https://gitee.com/half-lang/img/raw/master/202301192212683.png" alt="image-20230119221225658"></p><p>​ q={q1，…，qr}来表示查询图像中的密集特征。</p><p>​<img src="https://gitee.com/half-lang/img/raw/master/202301192215945.png" alt="image-20230119221524906" style="zoom: 50%;">将K个不同图像的相同空间位置的特征向量的平均为每个类别的支持密集特征。</p><p>​<strong>q和s</strong>表示一组特征向量，q和s表示单个特征向量。</p><p>​<img src="https://gitee.com/half-lang/img/raw/master/202301192218285.png" alt="image-20230119221823258" style="zoom: 50%;">所有支持类的稠密特征的并集。</p><h3 id="3-2-密度特征上的双向随机行走"><a href="#3-2-密度特征上的双向随机行走" class="headerlink" title="3.2.密度特征上的双向随机行走"></a>3.2.密度特征上的双向随机行走</h3><p>​<img src="https://gitee.com/half-lang/img/raw/master/202301192221931.png" alt="image-20230119222151902" style="zoom:50%;">表示v1，v2向量的余弦相似度</p><p>​给定两个密集特征集q和S，我们学习这两个不相交集之间的余弦相似矩阵Φ∈Rr×Nr，其中[Φ]qs=φ（q，S）。</p><p>​<img src="https://gitee.com/half-lang/img/raw/master/202301192226666.png" alt="image-20230119222638628" style="zoom:50%;">表示每个查询图像随机游走到支持图像的概率。可表示为<img src="https://gitee.com/half-lang/img/raw/master/202301192228723.png" alt="image-20230119222848679" style="zoom:50%;"></p><p>γ是softmax系数；D是对角归一化矩阵，元素为[Φ]qs</p><p>​<img src="https://gitee.com/half-lang/img/raw/master/202301192231786.png" alt="image-20230119223147755" style="zoom:50%;">同样用这个方法连接支持集到查询集的随机游走概率。</p><p>​假定一个粒子的游走概率遵从上述概率，我们可以用马尔科夫链表示：</p><p>​<img src="https://gitee.com/half-lang/img/raw/master/202301192234564.png" alt="image-20230119223448518" style="zoom:50%;"></p><p>​矩阵形式表示为：<img src="https://gitee.com/half-lang/img/raw/master/202301192235596.png" alt="image-20230119223552561" style="zoom:50%;"></p><p>​当无限次随机游走后粒子的分布将遵循：</p><p>​<img src="https://gitee.com/half-lang/img/raw/master/202301192237239.png" alt="image-20230119223717199"></p><p>​使用这些静态分布来编码相互关联的动机很简单：如果支持特征与查询特征具有相同的局部特征，那么在长时间的双向随机游走中，支持特征将经常被访问。换句话说，由于相互接近，可以预测与查询图像拥有最多相互关联的支持类。</p><p>​我们假设粒子在空间Z中均匀分布，根据在Xt内某个类被访问的数量表示类c为查询图像拥有的局部特征量：查询图像拥有的局部特征量：</p><p><img src="https://gitee.com/half-lang/img/raw/master/202301192249715.png" alt="image-20230119224911560"></p><p>​其中1[·]是一个指示函数，如果其参数为真，则等于1，否则为0。[·]ij表示从粒子j到粒子i的矩阵入口，[·]i表示与特征i相关的向量入口。</p><h3 id="3-4-用图论解释"><a href="#3-4-用图论解释" class="headerlink" title="3.4.用图论解释"></a>3.4.用图论解释</h3><p>​为了在图论中看到这一点，我们首先简要概述了特征向量中心性，该中心性反映了具有邻接{av1，v2}的从属网络中q∈q和s∈s的顶点v的得分x：</p><p>​<img src="https://gitee.com/half-lang/img/raw/master/202301201644144.png" alt="image-20230120164435079" style="zoom:50%;"></p><p>​V(·) 表示邻域</p><p>​<img src="https://gitee.com/half-lang/img/raw/master/202301201647761.png" alt="image-20230120164739693"></p><p>​引入了衰减因子α来表示中心性。</p><p><img src="https://gitee.com/half-lang/img/raw/master/202301201651747.png" alt="image-20230120165116691"></p><p>​图2:我们的三向单镜头分类框架。给定查询和支持图像，我们首先提取它们的密集特征q和S，这是两组特征向量。然后计算它们的相似矩阵Φ。之后，我们基于缩放和列归一化的相似度矩阵来构建概率矩阵P。我们通过方程中的Katz近似获得了平稳分布π（S）和π（q）。（12） 以及引理1。最后，我们将它们用于端到端FSL，并作为基于全局特征的FSL中的中心插件。我们探索了受[35]启发的三个密集特征提取器（即，VanillaFCN、PyramidFCN和PyramidGrid）来提取密集特征表示。</p><p>（fcn：全卷积神经网络）</p><p>我们在实验中探索了三个密集特征提取器，如图2所示：</p><p>（1）VanillaFCN简单地将全卷积网络的特征图输出视为密集特征。</p><p>（2） PyramidFCN使用额外的自适应平均池化层来获得每个图像的34个密集特征。</p><p>（3） PyramidGrid将图像均匀地裁剪成大小为2×2+3×3的网格，并将每个网格单元单独编码为特征向量。来自所有单元的特征向量构成密集特征集。</p><p>​由于MCL学习到的潜在中心性揭示了局部特征在从属网络上的不同重要性，因此，将其插入基于全局特征的方法中是可行的，方法是用中心性加权池代替它们的本地全局平均池（GAP），如下所示：</p><p>a.提取每个输入图像的密集特征。</p><p>b、 通过等式计算特征向量中心性xEigen。(12).</p><p> c、 根据引理1中单模中心性的定义计算π（S）和π（q）。</p><p>d、 使用π（q）作为查询密集特征的权重，并将其加权累积为单个特征向量。</p><p>e、 使用来自π（S）的逐类归一化中心性来累积每个支持类的密集特征。一旦我们获得了查询图像和支持类的中心性加权特征，就可以像以前一样直接执行传统的基于全局特征的方法。</p><h3 id="实验部分："><a href="#实验部分：" class="headerlink" title="实验部分："></a>实验部分：</h3><p>我们在两个广泛使用的FSL数据集和三个细粒度数据集上进行了实验：（1）miniImageNet[30]在100个类中每个类包含600个图像。我们遵循[23]使用的分割法，分别采用64、16和20类进行训练/评估/测试；（2） tieredImageNet[22]比具有608个类的miniImageNet大得多。</p><p>我使用miniimagenet数据集：</p><p>具体处理如下：</p><p><img src="https://gitee.com/half-lang/img/raw/master/202301201724742.png" alt="image-20230120172442656"></p><p>预训练：</p><p><img src="https://gitee.com/half-lang/img/raw/master/202301201725212.png" alt="image-20230120172541169"></p><p><img src="https://gitee.com/half-lang/img/raw/master/202301201726066.png" alt="image-20230120172606025"></p><p>原论文是训练300个epoch，目前还没有训练完。</p><p>原论文结果分析：</p><p><img src="https://gitee.com/half-lang/img/raw/master/202301201729956.png" alt="image-20230120172950879"></p><p>​表1。miniImageNet和tieredImageNet上的镜头分类精度很少。10000集评估的95%置信区间均低于0.2。斜体字体的结果表明MCL作为插件的性能。粗体字体的结果分别是不同密集特征提取器的最佳结果。我们在统一框架中重新实施了我们的插件基线方法（即ProtoNet和RelationNet）以及具有竞争力的基于密集特征的方法（即DN4、DeepEMD和FRN），以防其性能在特定设置下不可用。†：在Conv-4主干上重新实现的结果，‡：在两个主干上重新实施的结果。</p><p><img src="https://gitee.com/half-lang/img/raw/master/202301201732339.png" alt="image-20230120173252293"></p><p><strong>通过修改局部特征在全局池化中的权重来得到更加鲁棒的特征表示</strong></p><p>​</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>长尾问题</title>
      <link href="/2022/12/30/2022-12-30-chang-wei-wen-ti/"/>
      <url>/2022/12/30/2022-12-30-chang-wei-wen-ti/</url>
      
        <content type="html"><![CDATA[<h1 id="长尾问题"><a href="#长尾问题" class="headerlink" title="长尾问题"></a>长尾问题</h1><p>​在传统的分类任务中，训练数据往往都受到了人工的均衡，即不同类别样本数量没有明显差距。均衡的数据库将极大程度上简化了算法鲁棒性的要求，也一定程度上保障了模型的可靠性，但随着样本类别的增加维持各个类别之间的均衡将带来更大的成本。</p><p>​尤其是对于珍惜动物、与医疗上的罕见病例，部分类别的样本个数极为稀少，获取均衡的数据库几乎不可能。</p><p>​<img src="https://img-blog.csdnimg.cn/1229308b7cad4c33bc6c9a5070006e0b.png#pic_center" alt="img"></p><p>​</p><p>​目前解决长尾问题的解决方案有两种：重采样与重加权。</p><p>​重采样：<img src="/.com//Users\de'l'l\AppData\Roaming\Typora\typora-user-images\image-20221130201514898.png" alt="image-20221130201514898"></p><p>​其中C是类别数量，i为样本总数。pj为从j类样本中取样的概率，q为指数。</p><p>​重加权：</p><p>​重加权反应在loss值上，由于loss的计算灵活性，重加权的计算更加简单。在此仅列举通用公式：</p><p>​<img src="/.com//Users\de'l'l\AppData\Roaming\Typora\typora-user-images\image-20221130201924508.png" alt="image-20221130201924508"></p><p><a href="https://blog.csdn.net/weixin_42437114/article/details/120439298">(73条消息) Long-tailed Recognition (长尾问题)_连理o的博客-CSDN博客_长尾问题</a></p><p>​分类损失函数（Lcls）</p><h2 id="数据增广："><a href="#数据增广：" class="headerlink" title="数据增广："></a>数据增广：</h2><p>​图像数据准备对<a href="https://so.csdn.net/so/search?q=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;spm=1001.2101.3001.7020">神经网络</a>与卷积神经网络模型训练有重要影响，<strong>当样本空间不够或者样本数量不足的时候会严重影响训练或者导致训练出来的模型泛化程度不够，识别率与准确率不高</strong>！本文将会带你学会如何对已有的图像数据进行<a href="https://so.csdn.net/so/search?q=%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA&amp;spm=1001.2101.3001.7020">数据增强</a>，获取样本的多样性与数据的多样性从而为训练模型打下良好基础。在不改变图像类别的情况下，增加数据量，能提高模型的泛化能力。<br>​    不同的视角，不同的大小，物体的形变问题，物体的遮挡问题，光照条件，背景复杂的问题，每一类中有多种形态的问题。<br>​而数据增广的思路也就是解决这个问题。数据增广如何增广就要从实际的问题出发，比如医学的图片基本上拍摄的时候视角是固定的，所以就不需要不同视角的增广。木纹检测中视角是不固定的，就需要不同的视角，不同的大小的增广，还需要应不同的光照条件对数据进行增广。</p><p>​如<strong>GistNet: a Geometric Structure Transfer Network for Long-Tailed Recognition</strong>文中使用了特殊的归一化思路，以将尾样本数据的方差逼近head样本。</p><p>​</p>]]></content>
      
      
      <categories>
          
          <category> 论文理论预读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文理论预读 </tag>
            
            <tag> 长尾问题 </tag>
            
            <tag> 矮半截 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>全连接条件随机场</title>
      <link href="/2022/12/27/quan-lian-jie-tiao-jian-sui-ji-chang/"/>
      <url>/2022/12/27/quan-lian-jie-tiao-jian-sui-ji-chang/</url>
      
        <content type="html"><![CDATA[<h1 id="全连接条件随机场"><a href="#全连接条件随机场" class="headerlink" title="全连接条件随机场"></a>全连接条件随机场</h1><h2 id="前置知识："><a href="#前置知识：" class="headerlink" title="前置知识："></a>前置知识：</h2><h3 id=""><a href="#" class="headerlink" title=""></a></h3><h3 id="随机场-random-field"><a href="#随机场-random-field" class="headerlink" title="随机场 (random field)"></a>随机场 (random field)</h3><p>由若干位置组成的整体，每一个位置按某种分布随机地赋一个值，全体即组成一个随机场。</p><h3 id="马尔科夫随机场-MRF"><a href="#马尔科夫随机场-MRF" class="headerlink" title="马尔科夫随机场(MRF)"></a>马尔科夫随机场(MRF)</h3><p>马尔科夫随机场是随机场的特例，假设某一个位置的赋值只与和它相邻的位置相关。</p><h3 id="条件随机场-CRF"><a href="#条件随机场-CRF" class="headerlink" title="条件随机场(CRF)"></a>条件随机场(CRF)</h3><p>条件随机场是马尔科夫随机场的特例，假设马尔可夫随机场只有X和Y两个随机变量，一般情况下，X是给定的，Y是输出。<br>  形式化定义：设X和Y是随机变量，P ( Y ∣ X ) P(Y|X)P(Y∣X)是给定X时Y的条件概率分布，若Y构成一个马尔科夫随机场，则P ( Y ∣ X ) P(Y|X)P(Y∣X)是条件随机场。</p><h2 id="全连接条件随机场-1"><a href="#全连接条件随机场-1" class="headerlink" title="全连接条件随机场"></a>全连接条件随机场</h2><p>如下图，有一幅图像：</p><p><img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20221229224411.png"></p><p>如果说该图像上一点x的值仅跟x的位置相关，我们可以得到如下条件随机场：</p><p><img src="/.com//github博客\cqu-student.github.io\source_posts\全连接条件随机场\20221229224411.png" alt="20221229224411"></p><p>更加复杂地，每一个像素类别和四邻域像素有关，则如下图：</p><p><img src="/.com//github博客\cqu-student.github.io\source_posts\全连接条件随机场\20221229224411-16723259093292.png" alt="20221229224411"></p><p>更复杂的，将四邻域像素全连接的所有关系当作x：</p><p><img src="/.com//github博客\cqu-student.github.io\source_posts\全连接条件随机场\20221229224411-16723260342974.png" alt="20221229224411"></p><p>其中y是如像素颜色的属性。</p>]]></content>
      
      
      <categories>
          
          <category> 基础知识补足 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文理论预读 </tag>
            
            <tag> 矮半截 </tag>
            
            <tag> 12月31日组会 </tag>
            
            <tag> crf </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语义分割相关</title>
      <link href="/2022/12/27/yu-yi-fen-ge-xiang-guan/"/>
      <url>/2022/12/27/yu-yi-fen-ge-xiang-guan/</url>
      
        <content type="html"><![CDATA[<h1 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a>语义分割</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>语义分割，旨在将图像中的所有像素进行分类，一直是计算机视觉图像领域的主要任务之一。在实际应用中，由于能准确地定位到物体所在区域并以像素级的精度排除掉背景的影响，一直是精细化识别、图像理解的可靠方式。</p><p>但是，构建语义分割数据集需要对每张图像上的每个像素进行标注。据统计，单张1280*720像素的图像分割标注时间约1.5个小时[1]，而动辄上万、十万才能产生理想效果的数据集标注所需要的人力物力让实际业务项目投入产出比极低。</p><p>针对这个问题，仅需图像级标注即可达到接近的分割效果的弱监督语义分割是近年来语义分割相关方向研究的热点。该技术通过利用更简单易得的图像级标注，以训练分类模型的方式获取物体的种子分割区域并优化，从而实现图像的像素级密集性预测。</p><h2 id="弱监督语义分割"><a href="#弱监督语义分割" class="headerlink" title="弱监督语义分割"></a>弱监督语义分割</h2><ol><li><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>​    根据弱监督信号的形式，常见的弱监督语义分割可分为以下四类（图1）：</p><pre><code>① 图像级标注：仅标注图像中相关物体所属的类别，是最简单的标注；② 物体点标注：标注各个物体上某一点，以及相应类别；③ 物体框标注：标注各个物体所在的矩形框，以及相应类别；④ 物体划线标注：在各个物体上划一条线，以及相应类别。</code></pre><p><img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20221227214715.png"></p></li></ol><h3 id="2-基于图像级标注的弱监督语义分割步骤"><a href="#2-基于图像级标注的弱监督语义分割步骤" class="headerlink" title="2. 基于图像级标注的弱监督语义分割步骤"></a>2. 基于图像级标注的弱监督语义分割步骤</h3><p>​    基于图像级标注的弱监督语义分割大多采用多模块串联的形式进行，如图</p><p><img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20221227215036.png"></p><p>​首先，利用图像级标注的图像类别标签通过单标签或多标签分类的方式训练出一个分类模型，该分类模型通过计算图像中相应类别的类别特征响应图CAM[3]来当作分割伪标签的种子区域；接着，使用优化算法（如CRF[4]、AffinityNet[5]等）优化和扩张种子区域，获得最终的像素级的分割伪标签；最后，使用图像数据集和分割伪标签训练传统的分割算法（如Deeplab系列[6]）</p><h2 id="3-CAM"><a href="#3-CAM" class="headerlink" title="3.CAM"></a>3.CAM</h2><p>CAM（Class Activation Mapping）<br>        这篇文章是由周博磊在2016年的CVPR提出，作者发现了即使在没有定位标签的情况下训练好的CNN中间层也具备目标定位的特性，但是这种特性被卷积之后的向量拉伸和连续的全连接层破坏，但若是将最后的多个全连接层换成了全局平均池化层GAP和单个后接Softmax的全连接层，即可保留这种特性。同时，经过简单的计算，可以获取促使CNN用来确认图像属于某一类别的具有类别区分性的区域，即CAM。</p><p><img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20221229221446.png"></p><p>​其中，CAM的具体计算方式如下（如图3所示）：</p><p>​设fk(x,y)为最后一层卷积层获取的第k个特征图在(x,y)位置的值，wkc是类别c对应最后一层全连接层第k个权重，则类别c的响应特征图CAM在(x,y)位置的值为：</p><p><img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20221229221554.png"></p><p>​Mc即为CAM。最终CAM的值越大，表示对分类贡献度越高：如图3最后一幅图的热力图红色区域表示CAM值最大，也正是澳洲犬脸部区域。</p><p>​在文章中，作者表示CAM所在区域可直接作为弱监督目标定位的预测，并进行了相关实验，不仅相比当时最好的弱监督定位算法效果提升明显，而且仅需单次前向推理过程即可得到定位框。<br>————————————————</p><p>参考来源：<a href="https://blog.csdn.net/netease_im/article/details/123716966">(81条消息) 弱监督语义分割：从图像级标注快进到像素级预测_网易智企的博客-CSDN博客</a></p><h2 id="4-Grad-CAM"><a href="#4-Grad-CAM" class="headerlink" title="4.Grad-CAM"></a>4.Grad-CAM</h2><p>与cam相似，通过Grad-CAM我们能够绘制出如下的热力图（对应给定类别，网络到底关注哪些区域）。但cam仍有部分缺陷即需要修改网络结构并且重新训练，Grad-CAM完美避开了这些问题。</p><p><img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20221229222018.png"></p><p>作者训练了一个二分类网络，Nurse和Doctor。如下图所示，第一列是预测时输入的原图，第二列是Biased model（具有偏见的模型）通过Grad-CAM绘制的热力图。第三列是Unbiased model（不具偏见的模型）通过Grad-CAM绘制的热力图。通过对比发现，Biased model对于Nurse（护士）这个类别关注的是人的性别，可能模型认为Nurse都是女性，很明显这是带有偏见的。比如第二行第二列这个图，明明是个女Doctor（医生），但Biased model却认为她是Nurse（因为模型关注到这是个女性）。而Unbiased model关注的是Nurse和Doctor使用的工作器具以及服装，明显这更合理。</p><p><img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20221229222222.png"></p><p>​与cam不同的是：grad-cam使用的不是最后一层卷积输出的数据而是对每个点的数据关于最终权重y求一个偏导。将整个图片的偏导求和后与图片相乘得到输出结果。</p><p>​详细公式如下：</p><p>​<img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20221229223009.png"></p><p><img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20221229223037.png"></p><p>下面链接里作者给出了具体的例子与代码！</p><p>————————————————<br>原文链接：<a href="https://blog.csdn.net/qq_37541097/article/details/123089851">https://blog.csdn.net/qq_37541097/article/details/123089851</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文理论预读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文理论预读 </tag>
            
            <tag> 矮半截 </tag>
            
            <tag> 语义分割 </tag>
            
            <tag> 12月31日组会 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对抗学习相关</title>
      <link href="/2022/12/15/dui-kang-xue-xi-xiang-guan/"/>
      <url>/2022/12/15/dui-kang-xue-xi-xiang-guan/</url>
      
        <content type="html"><![CDATA[<h1 id="对抗学习"><a href="#对抗学习" class="headerlink" title="对抗学习"></a>对抗学习</h1><h2 id="对抗学习的基本概念"><a href="#对抗学习的基本概念" class="headerlink" title="对抗学习的基本概念"></a>对抗学习的基本概念</h2><p>​</p><p>什么是对抗学习？<br>机器学习这一技术自出现之始就以优异的性能应用于各个领域。近年来，随着机器学习的快速发展与广泛应用，这一领域更是得到前所未有的蓬勃发展。</p><p>目前, 机器学习在计算机视觉、语音识别、自然语言处理等复杂任务中取得了公认的成果，已经被广泛应用于自动驾驶、人脸识别等领域。随着机器学习技术遍地开花，逐渐深入人们的生活，其也被应用在许多例如安防、金融、医疗等对安全有严格要求的领域中，直接影响着人们的人身、财产和隐私的安危。</p><p>在一系列重大进展面前, 机器学习在越来越多的被应用到人类生活的方方面面，在这时人们也很容易忽视阳光背后的阴影。与很多实用性技术一样，机器学习作为一个复杂的计算机系统，同样面临着安全性的考验，同样会面临黑客攻击，也被发现存在着安全性问题，它们干扰机器学习系统输出正确结果，例如对抗样本（adversarial data）的存在。</p><p>研究人员发现，一些精心设计的对抗样本可以使机器学习模型输出错误的结果。</p><p><img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20210324141217456.jpg"></p><p>如图所示，一张正常的大熊猫图片（左图）在被加入噪声后生成对抗样本（右图），会使得神经网络分类错误，并被神经网络认为是一张长臂猿图片，然而人类并不能察觉到这种变化。[1]</p><p>对抗样本是指将真实的样本添加扰动而合成的新样本，是由深度神经网络的输入的数据和人工精心设计好的噪声合成得到的，但它不会被人类视觉系统识别错误。然而在对抗数据面前，深度神经网络却是脆弱的，可以轻易迷惑深度神经网络。</p><p>深度神经网络对于对抗样本的判错率非常高，而人几乎无法辨别原样本与对抗样本的差别，这就意味着原本深度神经网络的功能已经失效了，不具有媲美人类的智能了。</p><p>由此可见，对抗攻击的危害很大，尤其是对于无人驾驶、医疗诊断、金融分析这些安全至关重要的领域。对抗样本无疑制约着机器学习技术的进一步应用，因此，提升神经网络的对抗鲁棒性（抵御对抗样本的能力）变得十分重要。</p><p>于是，机器学习中的对抗样本引起了研究人员的极大关注，他们也相应提出了一系列的对抗攻击（如何更有效的产生对抗样本）和对抗防御（针对对抗样本提供更有效的防御）的方法，这一领域称之为对抗学习。对抗学习是一个机器学习与计算机安全的交叉领域，旨在于在恶意环境下（比如在对抗样本的存在的环境下）给机器学习技术提供安全保障。<br>————————————————<br>原文链接：<a href="https://blog.csdn.net/weixin_49669196/article/details/115175771">https://blog.csdn.net/weixin_49669196/article/details/115175771</a></p><h2 id="生成对抗网络（GAN）应用于图像分类"><a href="#生成对抗网络（GAN）应用于图像分类" class="headerlink" title="生成对抗网络（GAN）应用于图像分类"></a>生成对抗网络（GAN）应用于图像分类</h2><p>​与常规的深度学习模型（比如cnn、dbn、rnn）不同，GAN模型采用了两个独立的神经网络，分别称为“generator”和“discriminator”，生成器用于根据输入噪声信号生成‘看上去和真实样本差不多’的高维样本，判别器用于区分生成器产生的样本和真实的训练样本（属于一个二分类问题）。其模型结构框架如下，<br><img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20221216193103.png"></p><p>​GANs是基于一个minimax机制而不是通常的优化问题，它所定义的损失函数是关于判别器的最大化和生成器的最小化，作者也证明了GAN模型最终能够收敛，此时判别器模型和生成器模型分别取得最优解。记$x$表示样本数据，p(z)表示生成器的输入噪声分布，G(z; θ)表示噪声到样本空间的映射，D(x)表示x属于真实样本而不是生成样本的概率，那么GAN模型可以定义为如下的优化问题，</p><p><img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20221216193709.png"></p><pre><code>从以上公式可以看出，在模型的训练过程中，一方面需要修正判别器D，使值函数V最大化，也即使得D(x)最大化和D(G(z))最小化，其数学意义即最大化判别器分类训练样本和生成样本的正确率，另一方面需要修正生成器G，使值函数V最小化，也即使得D(G(z))最大化，其数学意义即生成器要尽量生成和训练样本非常相似的样本</code></pre><p>​早期的GAN模型主要应用于无监督学习任务，即生成和训练样本有相同分布的数据，可以为1维信号或者二维图像。将GAN应用于分类问题时，需要对网络做改动。</p><pre><code>将GAN应用于半监督分类任务时，只需要对最初的GAN的结构做稍微改动，即把discriminator模型的输出层替换成softmax分类器。假设训练数据有c类，那么在训练GAN模型的时候，可以把generator模拟出来的样本归为第c+1类，而softmax分类器也增加一个输出神经元，用于表示discriminator模型的输入为“假数据”的概率，这里的“假数据”具体指generator生成的样本。因为该模型可以利用有标签的训练样本，也可以从无标签的生成数据中学习，所以称之为“半监督”分类。</code></pre><p>可想而知，在应用于基于像素的有监督分类问题时（文章中的训练数据集类似于人脸识别数据集，区别在于单幅图像的标签y和输入人脸图像大小相同），GAN中的生成器模型是没有什么作用的。原作者所提出的网络框架包含了两个分类器模型，其中一个用于对单幅图像进行基于像素的分类，另外一个分类器也称作对抗网络，用于区分标签图和预测出来的概率图，引入对抗网络的目的是使得得到的概率预测图更符合真实的标签图，具体的网络结构如下，<br><img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20221216194626.png"></p><p>其中segmentor是一个分类器而adversarial是对抗网络，输入真实标签与分类器的概率图用于判断两者的区别，使用迭代优化训练两个网络。</p>]]></content>
      
      
      <categories>
          
          <category> 论文理论预读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文理论预读 </tag>
            
            <tag> 矮半截 </tag>
            
            <tag> 对抗学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无监督自适应学习相关</title>
      <link href="/2022/12/15/wu-jian-du-zi-gua-ying-xue-xi-xiang-guan/"/>
      <url>/2022/12/15/wu-jian-du-zi-gua-ying-xue-xi-xiang-guan/</url>
      
        <content type="html"><![CDATA[<h1 id="无监督自适应学习"><a href="#无监督自适应学习" class="headerlink" title="无监督自适应学习"></a>无监督自适应学习</h1><p><a href="https://zhuanlan.zhihu.com/p/50710267">《迁移学习》: 领域自适应(Domain Adaptation)的理论分析 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/SHU15121856/article/details/106874558">https://blog.csdn.net/SHU15121856/article/details/106874558</a></p><p><strong>首先Domain Adaptation基本思想是既然源域和目标域数据分布不一样，那么就把数据都映射到一个特征空间中，在特征空间中找一个度量准则，使得源域和目标域数据的特征分布尽量接近，于是基于源域数据特征训练的判别器，就可以用到目标域数据上。</strong></p><p>Domain Adaptation是一种源任务和目标任务一样，但是源域和目标域的数据分布不一样，并且源域有大量的标记好的样本，目标域则没有（或者只有非常少的）有标记的样本的迁移学习方法。这样就是怎么把源域上从大量的有标记样本中学习的知识迁移到目标域上，来解决相同的问题，而目标域上能利用的大多只有没有标记的样本。</p><p>这里要解释一下“数据分布不一样”是什么意思，就比如下图中(a)组是不同来源的自行车和笔记本电脑的照片，有从购物网站下载的，也有数码相机拍的生活照，也有网络上获取的照片等，它们虽然都表达自行车和笔记本电脑，但是数据分布是不同的。</p><p><img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20200620212935604.png"></p><p>比如用(b)组的门牌号数据集SVHN去训练模型，去提取SVNH和MNIST的特征，然后将其可视化到一个平面内，是下图左边的样子，蓝色点是源域（SVNH）的样本，红色的点是目标域（MNIST）的样本，也就是说直接在源域上训练得到的分类器的分类边界无法很好的区分目标域的样本。而领域自适应这种迁移学习方法想达到的效果就是下图右边这样，让源域和目标域中的样本能对齐，这样模型就能在目标域上很好的使用了。<br><img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20221216223738.png"></p><h2 id="基于对抗的自适应："><a href="#基于对抗的自适应：" class="headerlink" title="基于对抗的自适应："></a>基于对抗的自适应：</h2><p>如RevGrad（ICML,2015） 的基本思路就是用GAN去让生成器生成特征，然后让判别器判别它是源域的还是目标域的特征，如果判别不出来就说明在这个特征空间里源域和目标域是一致的。</p><p>下图中绿色部分是一个特征提取器，源域和目标域数据都扔进去，它就是用来生成（或者叫提取）特征的，然后紫色部分是对源域数据的特征做分类的分类器，红色部分是对源域数据和目标域数据的特征做判别的判别器，这个判别器要不断增强（能很好的判别是源域的还是目标域的特征），同时生成器也要增强，让生成出来的特征能混淆判别器的判别，这样最后生成（提取）出的特征就是源域和目标域空间里一致的了。</p><p><img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20200620233326328.png"></p><p>这个可以用GAN的最小化-最大化的思想去训练，也可以用论文中的梯度反转层（Gradient Reversal Layer） 的方法，就是在上图中白色空心箭头的位置加了个梯度反转层，在前向传播的过程中就是正常的网络，即最小化Loss让红色部分的判别器性能更好，再反向传播的过程中把梯度取负，即优化绿色部分的特征提取器，来尽量让红色部分的判别器分不清特征是源域的还是目标域的。这个方法就是一个训练技巧。</p><h2 id="课程学习"><a href="#课程学习" class="headerlink" title="课程学习"></a>课程学习</h2><p><a href="https://zhuanlan.zhihu.com/p/362351969">一篇综述带你全面了解课程学习(Curriculum Learning) - 知乎 (zhihu.com)</a></p><p>它是一种训练策略，<strong>模仿人类的学习过程，主张让模型先从容易的样本开始学习，并逐渐进阶到复杂的样本和知识</strong>。CL策略在计算机视觉和自然语言处理等多种场景下，在提高各种模型的泛化能力和收敛率方面表现出了强大的能力。</p><p><img src="https://cdn.jsdelivr.net/gh/half-lang/img-of-tian/20221216225442.png"></p><p>课程式学习是在 T个训练步骤上的训练标准序列 C=⟨Q1,…,Qt,…,QT⟩， 每个准则 Qt 是目标训练分布 P(z) 的权重。该准则包括数据，任务、模型容量、学习目标等。</p><h1 id="Transferable-curriculum-for-weakly-supervised-DA"><a href="#Transferable-curriculum-for-weakly-supervised-DA" class="headerlink" title="Transferable curriculum for weakly supervised DA"></a>Transferable curriculum for weakly supervised DA</h1><p><a href="https://zhuanlan.zhihu.com/p/57719459">Transferable curriculum for weakly supervised DA - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/weixin_43673376/article/details/108742246">(78条消息) 域适应系列2：Transferable Curriculum for Weakly-Supervised Domain Adaptation（AAAI 2019）_weixin_43673376的博客-CSDN博客_transferable curriculum for weakly-supervised doma</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文理论预读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文理论预读 </tag>
            
            <tag> 矮半截 </tag>
            
            <tag> 自适应学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>12月17日组会</title>
      <link href="/2022/12/15/2022-12-15-12-yue-17-ri-zu-hui/"/>
      <url>/2022/12/15/2022-12-15-12-yue-17-ri-zu-hui/</url>
      
        <content type="html"><![CDATA[<h1 id="Weakly-supervised-Temporal-Action-Localization-by-Uncertainty-Modeling"><a href="#Weakly-supervised-Temporal-Action-Localization-by-Uncertainty-Modeling" class="headerlink" title="Weakly-supervised Temporal Action Localization by Uncertainty Modeling"></a>Weakly-supervised Temporal Action Localization by Uncertainty Modeling</h1><h2 id="基于弱监督的视频动作检测"><a href="#基于弱监督的视频动作检测" class="headerlink" title="基于弱监督的视频动作检测"></a>基于弱监督的视频动作检测</h2><p>时序动作定位也称为时序动作检测。动作识别可以看作是一个纯分类问题，其中要识别的视频基本上已经过剪辑(Trimmed)，即每个视频包含一段明确的动作，视频时长较短，且有唯一确定的动作类别。而时序动作定位领域，视频通常没有被剪辑，视频时长较长，动作通常只发生在视频中的一小段时间内，视频可能包含多个动作，也可能不包含动作，即为背景类。时序动作定位不仅要预测视频中包含了什么动作，还要预测动作的起始和终止时刻。相比于动作识别，时序动作定位更接近现实场景。</p><p>时序动作定位可以看作由两个子任务组成，一个子任务是预测动作的起止时序区间，另一个子任务预测动作的类别。目前时序动作定位的关键是预测动作的起止时序区间。</p><p>时序动作检测的很多思路源于图像目标检测(Object Detection),了解目标检测的常见算法和关键思路对学习时序动作定位很有帮助。</p><ol><li><p>基于滑动窗口的算法<br> 思路为预先定义一系列不同时长的滑动窗，之后滑动窗在视频上沿着时间维度进行滑动，并逐一判断每个滑动窗对应的时序区间内具体是什么动作类别。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/cqu-student/Img/v2-acc8b7f7a204428160047b13ec91dc9b_720w.webp"></div></li><li><p>基于候选时序区间的算法<br> 目标检测算法中的两阶段(Two-Stage)算法将目标检测分为两个阶段：第一个阶段产生图像中可能存在目标的候选区域，一般一张图像可以产生成百上千候选区域。第二阶段逐一判断每个候选区域的类别并对候选区域的边界进行修正。</p><p> 类比于两阶段的目标检测算法，基于候选时序区间的时序动作定位算法将整个过程分为两个阶段：第一阶段产生视频中动作可能发生的候选时序区间，第二阶段逐一判断每个候选时序区间的类别并对候选时序区间的边界进行修正。最终将两个阶段的预测结果结合起来，得到未被剪辑视频中动作的类别和起止时刻预测。</p></li></ol><div align="center"><img src="https://cdn.jsdelivr.net/gh/cqu-student/Img/v2-5156b1f2089ccbbc4ace282b7728b738_r.jpg"></div><ol start="3"><li><p>自底向上的时序动作定位算法<br> 首先局部预测视频动作开始和动作结束的时刻，之后将开始和结束时刻组合成候选时序区间，最后对每个候选时序区间进行类别预测。</p></li><li><p>对时序结构信息建模的算法<br> 算法不仅会预测候选时序区间内的动作类别，还会预测候选时序区间的完整性，这样可以避免时序区间不完整的情况。</p></li><li><p>单阶段算法<br> 没有单独的候选区域生成的步骤，直接从图像中预测。通常两阶段算法识别精度高，但是预测速度慢，单阶段算法识别精度略低，但是预测速度快。时序动作定位中也有一些算法采用了单阶段算法的策略。</p></li></ol><p>[1] (<a href="https://zhuanlan.zhihu.com/p/422235052#:~:text=%E6%97%B6%E5%BA%8F%E5%8A%A8%E4%BD%9C%E5%AE%9A%E4%BD%8D%20%E6%97%B6%E5%BA%8F%E5%8A%A8%E4%BD%9C%E5%AE%9A%E4%BD%8D%20%28Temporal%20Action%20Localization%29,%E4%B9%9F%E7%A7%B0%E4%B8%BA%E6%97%B6%E5%BA%8F%E5%8A%A8%E4%BD%9C%E6%A3%80%E6%B5%8B%20%28Temporal%20Action%20Detection%29%EF%BC%8C%E6%98%AF%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E7%9A%84%E5%8F%A6%E4%B8%80%E4%B8%AA%E9%87%8D%E8%A6%81%E9%A2%86%E5%9F%9F%E3%80%82%20%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB%E5%8F%AF%E4%BB%A5%E7%9C%8B%E4%BD%9C%E6%98%AF%E4%B8%80%E4%B8%AA%E7%BA%AF%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%EF%BC%8C%E5%85%B6%E4%B8%AD%E8%A6%81%E8%AF%86%E5%88%AB%E7%9A%84%E8%A7%86%E9%A2%91%E5%9F%BA%E6%9C%AC%E4%B8%8A%E5%B7%B2%E7%BB%8F%E8%BF%87%E5%89%AA%E8%BE%91%20%28Trimmed%29%EF%BC%8C%E5%8D%B3%E6%AF%8F%E4%B8%AA%E8%A7%86%E9%A2%91%E5%8C%85%E5%90%AB%E4%B8%80%E6%AE%B5%E6%98%8E%E7%A1%AE%E7%9A%84%E5%8A%A8%E4%BD%9C%EF%BC%8C%E8%A7%86%E9%A2%91%E6%97%B6%E9%95%BF%E8%BE%83%E7%9F%AD%EF%BC%8C%E4%B8%94%E6%9C%89%E5%94%AF%E4%B8%80%E7%A1%AE%E5%AE%9A%E7%9A%84%E5%8A%A8%E4%BD%9C%E7%B1%BB%E5%88%AB%E3%80%82">https://zhuanlan.zhihu.com/p/422235052#:~:text=%E6%97%B6%E5%BA%8F%E5%8A%A8%E4%BD%9C%E5%AE%9A%E4%BD%8D%20%E6%97%B6%E5%BA%8F%E5%8A%A8%E4%BD%9C%E5%AE%9A%E4%BD%8D%20%28Temporal%20Action%20Localization%29,%E4%B9%9F%E7%A7%B0%E4%B8%BA%E6%97%B6%E5%BA%8F%E5%8A%A8%E4%BD%9C%E6%A3%80%E6%B5%8B%20%28Temporal%20Action%20Detection%29%EF%BC%8C%E6%98%AF%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E7%9A%84%E5%8F%A6%E4%B8%80%E4%B8%AA%E9%87%8D%E8%A6%81%E9%A2%86%E5%9F%9F%E3%80%82%20%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB%E5%8F%AF%E4%BB%A5%E7%9C%8B%E4%BD%9C%E6%98%AF%E4%B8%80%E4%B8%AA%E7%BA%AF%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%EF%BC%8C%E5%85%B6%E4%B8%AD%E8%A6%81%E8%AF%86%E5%88%AB%E7%9A%84%E8%A7%86%E9%A2%91%E5%9F%BA%E6%9C%AC%E4%B8%8A%E5%B7%B2%E7%BB%8F%E8%BF%87%E5%89%AA%E8%BE%91%20%28Trimmed%29%EF%BC%8C%E5%8D%B3%E6%AF%8F%E4%B8%AA%E8%A7%86%E9%A2%91%E5%8C%85%E5%90%AB%E4%B8%80%E6%AE%B5%E6%98%8E%E7%A1%AE%E7%9A%84%E5%8A%A8%E4%BD%9C%EF%BC%8C%E8%A7%86%E9%A2%91%E6%97%B6%E9%95%BF%E8%BE%83%E7%9F%AD%EF%BC%8C%E4%B8%94%E6%9C%89%E5%94%AF%E4%B8%80%E7%A1%AE%E5%AE%9A%E7%9A%84%E5%8A%A8%E4%BD%9C%E7%B1%BB%E5%88%AB%E3%80%82</a>)</p><h2 id="uncertainty-不确定度"><a href="#uncertainty-不确定度" class="headerlink" title="uncertainty(不确定度)"></a>uncertainty(不确定度)</h2><p>&amp;emsp; &amp;emsp;随机不确定度(Aleatoric uncertaint),认知不确定度(Epistemic uncertainty)</p><p>&amp;emsp; &amp;emsp;aleatoric unceratinty主要出现在物体边缘和远处(d)。该uncertainty源于数据本身，主要是标注员对物体边缘标注的精度不够、远处物体成像质量较差导致。本质就是训练数据中的噪声，来源于数据收集/标注过程。这些噪声是随机的，而且是固定的。噪声越多，数据的不确定度越大。可以被测量，但是无法通过增加数据减少。</p><p>&amp;emsp; &amp;emsp;measure what you can’t understand from data:上图中标注不好的地方（边缘和远处）。我们无法从标注不够精确的数据中，学习出一个”可以预测精细的物体轮廓“的模型）</p><p>&amp;emsp; &amp;emsp;Epistemic uncertainty主要出现在model预测不好的地方。比如最后一行，模型对人行道的分割结果较差(c)，所以Epistemic uncertainty比较高(e)</p><h3 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h3><p>&amp;emsp; &amp;emsp; 深度学习中，建模不确定度需要用到Bayesian DeepLearning。从Bayesian的角度，深度学习训练的本质要求是求一个posterior distribution $P(W|D)$,其中W是参数，D是数据。根据bayes theorem，有$P(W|D) = \frac{P(D|W)P(W)}{P(D)}$,然而由于$P(D)$理论上代表真是的数据分布，无法获取;P(W)在神经网络中不存在，因为模型训练好之后，所有的参数都是确定的数，而不是分布，所以无法计算P(W)。<br>&amp;emsp; &amp;emsp; 因此使用bayes&amp;emsp;theorem的另外一个公式: $P(D) = \sum_{i}P(D|W_{i})P(W_{i}))$ 若知道所有W，就可以计算$P(D)$,但也是不可能的。因此用蒙特卡洛法多次采样逼近：多次采样W计算$P_{i}(D)$，得到P(D)的近似分布，进而得到$P(W|D)$的估计。具体来说，有3种方式:</p><ol><li>Ensembles：用类似bootstrap的方法，对数据集D，采样N次，用N次的结果分别训练模型，然后ensemble模型结果。这个方法的好处是接近真实的Monte-Carlo方法</li><li>MCDropout：在网络中加入Dropout层，在测试时也打开Dropout，让Dropout成为采样器。对采样N次的结果进行ensemble处理得到最后的uncertainty。这个方法的好处是不用做很多实验，节省成本，但是由于使用了Dropout，单次训练的时间会变长。</li><li>MCDropConnect：和加Dropout的思路差不多。不过这里不用加Dropout layer，而是通过随机drop connection，来达到随机采样的目的。</li></ol><p>[1] (<a href="https://zhuanlan.zhihu.com/p/166617220">https://zhuanlan.zhihu.com/p/166617220</a>)</p><h2 id="out-of-distribution"><a href="#out-of-distribution" class="headerlink" title="out-of-distribution"></a>out-of-distribution</h2><p>&amp;emsp; &amp;emsp; 测试一个机器学习的分类器是否有效，一个标准为统计或者对抗检测，测试样本与训练分布有足够的距离。例如在一个猫咪品种识别的任务中，不同的猫咪照片是”in-distrubution”样本，其他动物和不是动物的东西是”out-of-distrubution”。真实平时生活中，数据分布通常是随时间变化，一直跟踪所用数据的分布变化非常昂贵，所以要做的不是一直去维护训练数据，而是要能识别出那些超出模型能力的数据。</p><p>更具体见链接[2]讲得很好。</p><p>[2] (<a href="https://xie.infoq.cn/article/bfd620b621160a3161d6b893c">https://xie.infoq.cn/article/bfd620b621160a3161d6b893c</a>)</p><h2 id="frame-wise-帧级别"><a href="#frame-wise-帧级别" class="headerlink" title="frame-wise(帧级别)"></a>frame-wise(帧级别)</h2><p>&amp;emsp;&amp;emsp;语音是非平稳信号，通过分帧可以认为每帧信号近似为平稳信号，然后就可以在一帧上提取特征比如频谱。这种就叫做帧级别特征。segmental level的特征应该是指多帧特征，比如音素特征就是segmental level的，而utterance level的就是指一句话上的特征，就有很多帧，例如说话人的特征就是utterance level的。 你可以近似认为这三个特征所覆盖的语音长度是逐步增大的。视频帧应该同理。</p><p>[3] (<a href="https://www.zhihu.com/question/425497537/answer/1522570410">https://www.zhihu.com/question/425497537/answer/1522570410</a>)</p><h2 id="多实例学习（Multi-Instance-Learning"><a href="#多实例学习（Multi-Instance-Learning" class="headerlink" title="多实例学习（Multi-Instance Learning)"></a>多实例学习（Multi-Instance Learning)</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>&amp;emsp;&amp;emsp;MIL的数据集的单位是bag，以二分类为例，一个bag中包含多个instance，如果所有的instance都被标记为negative，则这个包就是negative，反之这个包为positive。 举例:设想有若干个人，每个人手上有一个钥匙串(bag)，串有若干个钥匙(instance)。已知某个钥匙串能否打开特定的一扇门(training set)。我们的任务是要学习到哪一串钥匙串能打开这扇门，以及哪个钥匙能打开这扇门。</p><p>&amp;emsp;&amp;emsp;预测等级可以分为包预测和示例预测。如图，对包预测而言，红色虚线和紫色虚线都是最优解。对示例预测，只有紫色虚线才是最优解。包预测任务的表现基本上代表不了示例预测的表现。两者最大的区别在于对示例预测错误所付出的代价。包预测只要有就对，示例预测必须全部正确。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/cqu-student/Img/v2-7dff0d27019136834a9780c8a5cb356f_720w.webp"></div><p>&amp;emsp;&amp;emsp; witness rate(WR): $WR = \frac{positive instances}{total instances}$ WR很高时，可以用常规的监督学习算法来训练。WR很低时，会影响到算法的表现。但是在多模态分类中会出现数据分布的问题，进一步了解可以看链接原文。</p><p>&amp;emsp;&amp;emsp;多模态分类(MIL)可以用于目标检测，由于包的类别不能由单独某个instance学习到,但是可以通过多个instance相互学习到，如检测沙滩，则必须要同时检测到沙和水。</p><p>&amp;emsp;&amp;emsp;在目标定位和分割中从包中学习对示例的分类。比如说，从弱标签数据集训练一个识别系统。弱标签数据集指，对单张图片来说，只标记这张图片里有哪些物体，而不标记这些物体对应的bounding box或者像素级的标记。在视频定位中，一段视频被分成若干段，这些片段被分别进行分类。</p><p>[4] (<a href="https://zhuanlan.zhihu.com/p/299819082">https://zhuanlan.zhihu.com/p/299819082</a>)</p><h2 id="特征嵌入"><a href="#特征嵌入" class="headerlink" title="特征嵌入"></a>特征嵌入</h2><p>&amp;emsp;&amp;emsp; 为了存储图像，计算机要存储三个独立的矩阵(矩阵可以理解为二维数组)，三个矩阵分别与此图像的红色、绿色和蓝色相对应。如果图像的大小是64<em>64个像素，则3个64</em>64大小的矩阵在计算机中代表了这张图像。为了便于后续处理，将3个矩阵转化为1个向量x。则向量x的总维数就是64<em>64</em>3，结果为12288。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/cqu-student/Img/dab9e87d5e864007968d44e3602eb193.png"></div>              <h1 id="Dual-Evidential-Learning-for-Weakly-supervised-Temporal-Action-Localization"><a href="#Dual-Evidential-Learning-for-Weakly-supervised-Temporal-Action-Localization" class="headerlink" title="Dual-Evidential Learning for Weakly-supervised Temporal Action Localization"></a>Dual-Evidential Learning for Weakly-supervised Temporal Action Localization</h1><h2 id="多标签分类-multi-label-classification"><a href="#多标签分类-multi-label-classification" class="headerlink" title="多标签分类(multi-label classification)"></a>多标签分类(multi-label classification)</h2><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><p>&amp;emsp;&amp;emsp;研究的为一个样本由一个样例和一个集合的标签组成。对于一个没见过的样本$x \in X$,多标签分类器$h(·)$预测$h(x)\in Y$作为该样本的标签集合。 </p><h3 id="任务分解"><a href="#任务分解" class="headerlink" title="任务分解"></a>任务分解</h3><p>&amp;emsp;&amp;emsp;MLL包括两个主要任务: 多标签分类(MLC)和标签排序(LR)。</p><p>给定一个样例，一个多标签分类器会返回一个相关的标签集合Y和Y的补集 ,即无关的标签集合。所以，也就产生了一个标签空间的二元划分。</p><p>LR定一个函数f:$f: X \times L -&gt; R$返回标签的相关性。</p><p>第三个任务，多标签排序，可以理解为MLC和LR的泛化。同时产生一个二元划分和一致性排序。</p><p>[3] (<a href="https://zhuanlan.zhihu.com/p/183957063">https://zhuanlan.zhihu.com/p/183957063</a>)</p><h2 id="EDL-Evidential-Deep-Learning-证据深度学习"><a href="#EDL-Evidential-Deep-Learning-证据深度学习" class="headerlink" title="EDL(Evidential Deep Learning) 证据深度学习"></a>EDL(Evidential Deep Learning) 证据深度学习</h2><p>&amp;emsp;&amp;emsp;针对的是不确定性估计问题。EDL的基本假设是，认为模型输出的分类概率服从迪利克雷分布，通过最大化观测数据的概率似然，直接估计迪利克雷分布参数。EDL方法使用深度神经网络DNN直接学习迪利克雷分布$D(p|\alpha)$的分布参数$\alpha \in R^{K},即\alpha^{*}=f(x)$,K类的分布概率$P\in R^{K}$于是被视为从分布$D(P|\alpha)$中的采样，即$P~D(p|\alpha)$。这样，EDL模型直接学习到了分类概率的不确定性，一种类似于熵的二阶不确定性。</p><h3 id="不确定性校准"><a href="#不确定性校准" class="headerlink" title="不确定性校准"></a>不确定性校准</h3><p>&amp;emsp;&amp;emsp;EDL的目标是一种最大似然估计，具有较高的过拟合风险。在不确定性估计方面，过拟合体现为无论样本是否未知，模型都过于信任分类结果。一个好的不确定性分类模型。其不确定性应当具备的特性是: 准确分类的样本具有较高的可信度(低不确定性),错误分类的样本具有较低的可信度(高不确定性)。</p><p>[4] (<a href="https://zhuanlan.zhihu.com/p/410855695">https://zhuanlan.zhihu.com/p/410855695</a>)</p><p>EDL的提出论文:</p><p>[5] (<a href="https://zhuanlan.zhihu.com/p/445915367">https://zhuanlan.zhihu.com/p/445915367</a>)</p><h2 id="非极大性抑制-non-maximum-suppression-NMS"><a href="#非极大性抑制-non-maximum-suppression-NMS" class="headerlink" title="非极大性抑制 non-maximum-suppression (NMS)"></a>非极大性抑制 non-maximum-suppression (NMS)</h2><p>&amp;emsp;&amp;emsp;抑制不是极大值的元素，可以理解为局部最大搜索。最简单的方法为判断当前元素是否大于其左邻与右邻元素，如果符合条件，则该元素为极大值点。如果不符合条件，则将其右邻[i+1]作为极大值候选，采用单调递增的方式向右查找，直至找到满足I[i]&gt;I[i+1]的元素。</p><div align="center"><img src="!https://cdn.jsdelivr.net/gh/cqu-student/Img/v2-d0ac5772941ae435e028cf7aa4d66e56_r.png"></div><p>目标检测中的非极大性抑制:</p><ol><li>将所有的框按类别划分，并剔除背景类，因为无需NMS。</li><li>对每个物体类中的边界框(B_BOX)，按照分类置信度降序排列。</li><li>在某一类中，选择置信度最高的边界框B_BOX1，将B_BOX1从输入列表中去除，并加入输出列表。</li><li>逐个计算B_BOX1与其余B_BOX2的交并比IoU，若IoU(B_BOX1,B_BOX2) &gt; 阈值TH，则在输入去除B_BOX2。</li><li>重复步骤3~4，直到输入列表为空，完成一个物体类的遍历。</li><li>重复2~5，直到所有物体类的NMS处理完成。</li><li>输出列表，算法结束</li></ol><h1 id="Uncertainty-Guided-Collaborative-Training-for-Weakly-Supervised-Temporal-Action-Detection"><a href="#Uncertainty-Guided-Collaborative-Training-for-Weakly-Supervised-Temporal-Action-Detection" class="headerlink" title="Uncertainty Guided Collaborative Training for Weakly Supervised Temporal Action Detection"></a>Uncertainty Guided Collaborative Training for Weakly Supervised Temporal Action Detection</h1><h2 id="伪标签"><a href="#伪标签" class="headerlink" title="伪标签"></a>伪标签</h2><p>&amp;emsp;&amp;emsp;通过一部分有标签的数据集进行网络训练，再使用训练的网络对无标签的数据集进行分类。这部分无标签由网络进行标注的数据集标签则称为伪标签。</p><h2 id="协作训练-collaboratively-train"><a href="#协作训练-collaboratively-train" class="headerlink" title="协作训练(collaboratively train)"></a>协作训练(collaboratively train)</h2><p>&amp;emsp;&amp;emsp; 一种多视角学习方法，与熟知的单视角不同。比如在网页分类问题中，网页拥有两个独立的视角即链接和网页内容。当数据充分时， 在具有这种特征的数据集的任何一个视图上均可以利用一定的机器学习算法训练出一个强分类器。因为 无论是链接还是网页内容都能独立完备地唯一确定一个网页。但是在这里，大量数据都是无标记的，无法训练出一个强泛化能力的分类器，怎么办呢？协同训练应运而生！ </p><p>&amp;emsp;&amp;emsp;假设数据集属性有两个充分冗余的视图view1和view2，设为X1和X2，则一个示例就可以表示为(x1, x2)，其中x1是x在X1视图中的特征向量，x2则是x在X2视图中的特征向量。假设f是在示例空间X中的目标函数，若x的标记为l则应有f(x) = f1(x1) = f2(x2) = l。A. Blum和T. Mitchell定义了所谓的“相容性”，即对X上的某个分布D，C1和C2分别是定义在X1和X2上的概念类，如果D对满足f1(x) ≠ f2(x2) 的示例 (x1, x2) 指派零概率，则称目标函数f = (f1, f2) ∈ C1 × C2与D“相容”。</p><p>&amp;emsp;&amp;emsp;这样就可以利用未标记示例来辅助探查哪些目标概念是相容的，该信息有助于减少学习算法所需的有标记示例数。当示例(x1,x2)在分布D下以 非零概率 存在时，结点x1和x2之间才存在边。当连接完边之后，属于同一连通成分的示例属于同样的类别，未标记示例可以帮助学习算法了解图中的连通性。因此，通过利用未标记示例，学习算法可以使用较少的有标记示例达到原来需要更多的有标记示例才能达到的效果。</p><p>标准协同训练算法的步骤为：</p><p>输入：标记数据集L，未标记数据集U。</p><ol><li>用L1训练视图X1上的分类器f1，用L2训练视图X2上的分类器f2；</li><li>用f1和f2分别对未标记数据U进行分类；</li><li>把f1对U的分类结果中，前k个最置信的数据（正例p个反例n个）及其分类结果加入L2；把f2对U的分类结果中，前k个最置信的数据及其分类结果加入L1；把这2（p+n）个数据从U中移除；</li><li>重复上述过程，直到U为空集。</li></ol><p>输出：分类器f1和f2。</p><p>[8] (<a href="https://blog.csdn.net/xiaopihaierletian/article/details/52757379">https://blog.csdn.net/xiaopihaierletian/article/details/52757379</a>)</p>]]></content>
      
      
      <categories>
          
          <category> 论文理论预读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -论文理论预读 -目标检测 -视频动作定位 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>12月31日组会</title>
      <link href="/2022/12/15/2022-12-31-12-yue-31-ri-zu-hui/"/>
      <url>/2022/12/15/2022-12-31-12-yue-31-ri-zu-hui/</url>
      
        <content type="html"><![CDATA[<h1 id="Dynamic-Prototype-Convolution-Network-for-Few-Shot-Semantic-Segmentation"><a href="#Dynamic-Prototype-Convolution-Network-for-Few-Shot-Semantic-Segmentation" class="headerlink" title="Dynamic Prototype Convolution Network for Few-Shot Semantic Segmentation"></a>Dynamic Prototype Convolution Network for Few-Shot Semantic Segmentation</h1><h2 id="meta-learning-strategy"><a href="#meta-learning-strategy" class="headerlink" title="meta-learning strategy"></a>meta-learning strategy</h2><p>&amp;ensp;元学习Meta Learning，含义为学会学习，即learn to learn，就是带着这种对人类这种“学习能力”的期望诞生的。Meta Learning希望使得模型获取一种“学会学习”的能力，使其可以在获取已有“知识”的基础上快速学习新的任务，</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/cqu-student/Img/v2-23b952fb974edeffa4e28d0065440227_720w.webp"></div><p>&amp;ensp;在机器学习中，训练单位是一条数据，通过数据来对模型进行优化；数据可以分为训练集、测试集和验证集。在元学习中，训练单位分层级了，第一层训练单位是任务，也就是说，元学习中要准备许多任务来进行学习，第二层训练单位才是每个任务对应的数据。</p><p>&amp;ensp;二者的目的都是找一个Function，只是两个Function的功能不同，要做的事情不一样。机器学习中的Function直接作用于特征和标签，去寻找特征与标签之间的关联；而元学习中的Function是用于寻找新的f，新的f才会应用于具体的任务。</p><p>[1] (<a href="https://zhuanlan.zhihu.com/p/136975128">https://zhuanlan.zhihu.com/p/136975128</a>)</p><h2 id="FSS"><a href="#FSS" class="headerlink" title="FSS"></a>FSS</h2><p>任务: 以one-shot为例，在support set 中，给定新类（比如狗）的一张图片（或多张图片，比如few-shot，就是多张）以及对应的分割mask（label），这里需要注意，这里的新类（狗）并没有出现在训练任务中。然后在quiery set 给定一张含新类（狗）的图片，需要准确地对这张图片进行分割。这个问题简单说就是：给你一张有mask的图片，比如图片中这是一只狗，然后给你其他新的图片，你还能知道哪个是狗并分割出来，这如何做到的呢？</p><p>训练: 训练的时候，模型已经学会了如何做分割的方式（可认为是先验知识或经验），当你做测试时候，只需要按学会的方式去分割就可以了（举一反三）。这跟迁移学习的区别在于：迁移学习需要在大量数据中先训练一个模型，做新任务时去Fine-tune就行，所以训练数据和测试数据需要相近。主要区别就是，当碰到新类别/没见过数据，前者直接按照训练学会的经验就能完成，后者要么去Fine-tune或重新训练。而Meta Learning 则可以很好的利用以往的知识，能根据新任务的调整自己。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/cqu-student/Img/20200731094509383.png"></div><p>[2] (<a href="https://blog.csdn.net/weixin_43673376/article/details/107705700">https://blog.csdn.net/weixin_43673376/article/details/107705700</a>)</p><h2 id="asymmetric-kernels"><a href="#asymmetric-kernels" class="headerlink" title="asymmetric kernels"></a>asymmetric kernels</h2><p>&amp;ensp;非对称卷积通常用于逼近现有的正方形卷积以进行模型压缩和加速，先前的一些工作表明，可以将标准的d×d卷积分解为1×d和d×1卷积，以减少参数量。其背后的理论相当简单：如果二维卷积核的秩为1，则运算可等价地转换为一系列一维卷积。然而，由于深度网络中下学习到的核具有分布特征值，其内在秩比实际中的高，因此直接将变换应用于核会导致显著的信息损失。</p><p>[3] (<a href="https://zhuanlan.zhihu.com/p/371099423#:~:text=%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8D%B7%E7%A7%AF%E9%80%9A%E5%B8%B8%E7%94%A8%E4%BA%8E%E9%80%BC%E8%BF%91%E7%8E%B0%E6%9C%89%E7%9A%84%E6%AD%A3%E6%96%B9%E5%BD%A2%E5%8D%B7%E7%A7%AF%E4%BB%A5%E8%BF%9B%E8%A1%8C%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%8A%A0%E9%80%9F%EF%BC%8C%E5%85%88%E5%89%8D%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B7%A5%E4%BD%9C%E8%A1%A8%E6%98%8E%EF%BC%8C%E5%8F%AF%E4%BB%A5%E5%B0%86%E6%A0%87%E5%87%86%E7%9A%84%20dtimes%20d%20%E5%8D%B7%E7%A7%AF%E5%88%86%E8%A7%A3%E4%B8%BA%201times,d%20%E5%92%8C%20dtimes%201%20%E5%8D%B7%E7%A7%AF%EF%BC%8C%E4%BB%A5%E5%87%8F%E5%B0%91%E5%8F%82%E6%95%B0%E9%87%8F%E3%80%82%20%E5%85%B6%E8%83%8C%E5%90%8E%E7%9A%84%E7%90%86%E8%AE%BA%E7%9B%B8%E5%BD%93%E7%AE%80%E5%8D%95%EF%BC%9A%E5%A6%82%E6%9E%9C%E4%BA%8C%E7%BB%B4%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E7%A7%A9%E4%B8%BA1%EF%BC%8C%E5%88%99%E8%BF%90%E7%AE%97%E5%8F%AF%E7%AD%89%E4%BB%B7%E5%9C%B0%E8%BD%AC%E6%8D%A2%E4%B8%BA%E4%B8%80%E7%B3%BB%E5%88%97%E4%B8%80%E7%BB%B4%E5%8D%B7%E7%A7%AF%E3%80%82">https://zhuanlan.zhihu.com/p/371099423#:~:text=%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8D%B7%E7%A7%AF%E9%80%9A%E5%B8%B8%E7%94%A8%E4%BA%8E%E9%80%BC%E8%BF%91%E7%8E%B0%E6%9C%89%E7%9A%84%E6%AD%A3%E6%96%B9%E5%BD%A2%E5%8D%B7%E7%A7%AF%E4%BB%A5%E8%BF%9B%E8%A1%8C%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%8A%A0%E9%80%9F%EF%BC%8C%E5%85%88%E5%89%8D%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B7%A5%E4%BD%9C%E8%A1%A8%E6%98%8E%EF%BC%8C%E5%8F%AF%E4%BB%A5%E5%B0%86%E6%A0%87%E5%87%86%E7%9A%84%20dtimes%20d%20%E5%8D%B7%E7%A7%AF%E5%88%86%E8%A7%A3%E4%B8%BA%201times,d%20%E5%92%8C%20dtimes%201%20%E5%8D%B7%E7%A7%AF%EF%BC%8C%E4%BB%A5%E5%87%8F%E5%B0%91%E5%8F%82%E6%95%B0%E9%87%8F%E3%80%82%20%E5%85%B6%E8%83%8C%E5%90%8E%E7%9A%84%E7%90%86%E8%AE%BA%E7%9B%B8%E5%BD%93%E7%AE%80%E5%8D%95%EF%BC%9A%E5%A6%82%E6%9E%9C%E4%BA%8C%E7%BB%B4%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E7%A7%A9%E4%B8%BA1%EF%BC%8C%E5%88%99%E8%BF%90%E7%AE%97%E5%8F%AF%E7%AD%89%E4%BB%B7%E5%9C%B0%E8%BD%AC%E6%8D%A2%E4%B8%BA%E4%B8%80%E7%B3%BB%E5%88%97%E4%B8%80%E7%BB%B4%E5%8D%B7%E7%A7%AF%E3%80%82</a>)</p><h1 id="MAXIM-Multi-Axis-MLP-for-Image-Processing"><a href="#MAXIM-Multi-Axis-MLP-for-Image-Processing" class="headerlink" title="MAXIM: Multi-Axis MLP for Image Processing"></a>MAXIM: Multi-Axis MLP for Image Processing</h1><h2 id="patch-boundary-artifacts"><a href="#patch-boundary-artifacts" class="headerlink" title="patch boundary artifacts"></a>patch boundary artifacts</h2><p>&amp;ensp;振铃效果是由锐利边缘产生的，如果以常数填补边界，则边界处会出现明显的“断层”，即锐利的边缘。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/cqu-student/Img/2020021318233097.png"></div><h2 id="low-level-vision"><a href="#low-level-vision" class="headerlink" title="low-level vision"></a>low-level vision</h2><p>&amp;ensp;Low-level feature:通常是指图像中的一些小的细节信息，例如边缘(edge)，角(corner),颜色(color),像素(pixeles),梯度(gradients)等，这些信息可以通过滤波器、SIFT或HOG获取。</p><p>&amp;ensp;除了High-level Vision任务外，很少有研究将transformer应用于low-level vision领域，例如图像超分辨率，图像生成等。与以标签或框为输出的分类，分割和检测相比，low-level vision任务 通常将图像作为输出（例如，高分辨率图像或去噪图像），这更具有挑战性。</p><p>[4] (<a href="https://www.ebaina.com/articles/140000012535#:~:text=%E4%BD%8E%E7%BA%A7%E8%A7%86%E8%A7%89%EF%BC%9A%E4%B8%BB%E8%A6%81%E5%85%B3%E6%B3%A8%E7%9A%84%E6%98%AF%E4%BB%8E%E8%A7%86%E7%BD%91%E8%86%9C%E5%9B%BE%E5%83%8F%E4%B8%AD%E6%8F%90%E5%8F%96%E5%9B%BE%E5%83%8F%E7%9A%84%E7%89%B9%E6%80%A7%E3%80%82%20middle-level%20vision%20middle-level%20vision%3A%20which%20concerns,the%20integration%20of%20image%20properties%20into%20perceptual%20organizations">https://www.ebaina.com/articles/140000012535#:~:text=%E4%BD%8E%E7%BA%A7%E8%A7%86%E8%A7%89%EF%BC%9A%E4%B8%BB%E8%A6%81%E5%85%B3%E6%B3%A8%E7%9A%84%E6%98%AF%E4%BB%8E%E8%A7%86%E7%BD%91%E8%86%9C%E5%9B%BE%E5%83%8F%E4%B8%AD%E6%8F%90%E5%8F%96%E5%9B%BE%E5%83%8F%E7%9A%84%E7%89%B9%E6%80%A7%E3%80%82%20middle-level%20vision%20middle-level%20vision%3A%20which%20concerns,the%20integration%20of%20image%20properties%20into%20perceptual%20organizations</a>.)</p><h1 id="Structure-Boundary-Preserving-Segmentation-for-Medical-Image-with-Ambiguous-Boundary"><a href="#Structure-Boundary-Preserving-Segmentation-for-Medical-Image-with-Ambiguous-Boundary" class="headerlink" title="Structure Boundary Preserving Segmentation for Medical Image with Ambiguous Boundary"></a>Structure Boundary Preserving Segmentation for Medical Image with Ambiguous Boundary</h1><h2 id="automatic-medical-segmentation-自动医疗图像分割"><a href="#automatic-medical-segmentation-自动医疗图像分割" class="headerlink" title="automatic medical segmentation(自动医疗图像分割)"></a>automatic medical segmentation(自动医疗图像分割)</h2><p>&amp;ensp;医学图像分割的目的是使图像中解剖或病理结构的变化更加清晰;目前流行的医学图像分割任务包括肝脏和肝脏肿瘤分割，脑和脑肿瘤分割，视盘分割，细胞分割，肺分割和肺结节等。为了帮助临床医生做出准确的诊断，有必要对医学图像中的一些关键目标进行分割，并从分割区域中提取特征。但是该领域的难点为$医学图像的特征提取比普通RGB图像更难，往往存在模糊、噪声、对比度低等问题$</p><p>&amp;ensp; 与与普通的图像分割不同，医学图像通常含有噪声且边界模糊。因此，仅仅依靠图像的底层特征很难对医学图像中的目标进行检测和识别。同时，由于缺乏图像细节信息，仅靠图像语义特征无法获得准确的边界。而U-Net通过跳跃连接，将低分辨率和高分辨率的特征图结合起来，有效地融合了低分辨率和高分辨率的图像特征，是医学图像分割任务的完美解决方案。目前，U-Net已经成为大多数医学图像分割任务的基准，并激发了许多有意义的改进。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/cqu-student/Img/20210331175558832.png"></div><h2 id="Interactive-Segmentation"><a href="#Interactive-Segmentation" class="headerlink" title="Interactive Segmentation"></a>Interactive Segmentation</h2><p>&amp;ensp;有效的交互分割的关键是，临床医生可以使用交互方法，如鼠标点击和轮廓框，以而改善从模型输出的初始分割结果。然后，该模型可以更新参数并生成新的分割图像，再获得临床医生的新的反馈。</p><p>[5] (<a href="https://blog.csdn.net/qq_38932073/article/details/115354406">https://blog.csdn.net/qq_38932073/article/details/115354406</a>)</p>]]></content>
      
      
      <categories>
          
          <category> 论文理论预读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文理论预读 </tag>
            
            <tag> 高一头 </tag>
            
            <tag> 语义分割 </tag>
            
            <tag> 小样本语义分割 </tag>
            
            <tag> 12月31日组会 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>12月1日组会</title>
      <link href="/2022/12/04/2022-12-4-12-yue-1-ri-zu-hui/"/>
      <url>/2022/12/04/2022-12-4-12-yue-1-ri-zu-hui/</url>
      
        <content type="html"><![CDATA[<h1 id="Large-Scale-Pre-training-for-Person-Re-identification-with-Noisy-Labels"><a href="#Large-Scale-Pre-training-for-Person-Re-identification-with-Noisy-Labels" class="headerlink" title="Large-Scale Pre-training for Person Re-identification with Noisy Labels"></a>Large-Scale Pre-training for Person Re-identification with Noisy Labels</h1><p>领域: 目标检测 行人重识别</p><h2 id="Noisy-Labels"><a href="#Noisy-Labels" class="headerlink" title="Noisy-Labels"></a>Noisy-Labels</h2><div></div>  在数据集中，一些样本的类别被错误的标注，比如猫狗分类集中猫被标注成了狗狗。这些样本被称为**noisy label**。与此相对的我们还经常听到**hard Sample**。**hard Sample**用来描述标注正确但是模型难以学习的样本，比如一只很像猫的狗，虽然难以分辨,但是它的标签是正确的。如果一只猫的标签是狗 那么它就是一个noisy label.<h2 id="Tracklet"><a href="#Tracklet" class="headerlink" title="Tracklet"></a>Tracklet</h2><div></div>  跟踪小片段，物体跟踪时会用到数据关联，整个连续的跟踪过程其实为由很多tracklet构成的，一般为5-6帧的集合。<h2 id="Re-ID-learning"><a href="#Re-ID-learning" class="headerlink" title="Re-ID learning"></a>Re-ID learning</h2><div></div>  行人重识别（Person Re-identification也称行人再识别，简称为ReID，是利用计算机视觉技术判断图像或者视频序列中是否存在特定行人的技术；或者说，行人重识别是指在已有的可能来源与非重叠摄像机视域的视频序列中识别出目标行人。广泛被认为是一个图像检索的子问题。给定一个监控行人图像，检索跨设备下的该行人图像。在监控视频中，由于相机分辨率和拍摄角度的缘故，通常无法得到质量非常高的人脸图片。当人脸识别失效的情况下，ReID就成为了一个非常重要的替代品技术。ReID有一个非常重要的特性就是跨摄像头，所以学术论文里评价性能的时候，是要检索出不同摄像头下的相同行人图片。            <div align="center">                <img src="https://pic1.zhimg.com/v2-0c1a1b7dcec05fd4f298d3c5a3149f7f_720w.jpg?source=d16d100b">            </div><div></div>  常用方法:1. 基于表征学习的ReID方法:            <div align="center">                <img src="https://pic3.zhimg.com/80/v2-ee7edda90bd5e1a5f97cb83f79d6dda6_720w.webp">            </div>2. 基于度量学习的ReID方法:<br>   <div></div>  不同于特征学习，度量学习旨在通过网络学习出两张图片的相似度3. 基于局部特征的ReID方法:<br>   <div></div>  常用的提取局部特征的思路主要有图像切块、利用骨架关键点定位以及姿态矫正等等。（1）图片8切块是一种很常见的提取局部特征方式。如下图所示，图片被垂直等分为若干份，因为垂直切割更符合我们对人体识别的直观感受，所以行人重识别领域很少用到水平切割。   <div align="center">                <img src="https://pic4.zhimg.com/80/v2-7f318fbd8acd1736871c3268e844a8d3_720w.webp">    </div>4. 基于视频序列的ReID方法:<div></div>  基于视频序列的方法最主要的不同点就是这类方法不仅考虑了图像的内容信息，还考虑了帧与帧之间的运动信息等。代表方法:AMOC<br><div></div>  基于单帧图像的方法主要思想是利用CNN来提取图像的空间特征，而基于视频序列的方法主要思想是利用CNN 来提取空间特征的同时利用递归循环网络(Recurrent neural networks, RNN)来提取时序特征。<br><div align="center">                <img src="https://pic3.zhimg.com/80/v2-548b4735788e96cec985c6644cd5588e_720w.webp">    </div><div></div>  AMOC:输入 原始的图像序列和提取的光流序列<br><div></div>  核心思想: 网络除了要提取序列图像的特征，还要提取运动光流的运动特征。<div></div>  网络内容: 空间信息网络 + 运动信息网络<div align="center">                <img src="https://pic2.zhimg.com/80/v2-ca257bd81d5f9d724a8820af211183e5_720w.webp"></div><p>评价指标:</p><div></div>  **ROC**: ROC曲线是检测、分类、识别任务中很常用的一项评价指标。曲线上每个点反映着对同一信号刺激的感受性。具体到识别任务中就是，ROC曲线上的每一点反映的是不同的阈值对应的FP（false positive）和TP（true positive）之间的关系,如图所示。<div align="center">                <img src="https://pic1.zhimg.com/80/v2-02a39384b6f0e6e6520538532a047998_720w.webp"></div>       <b>CMC</b>:    CMC曲线是算一种top-k的击中概率，主要用来评估闭集中rank的正确率。[m1,m2,m3,m4,m5]为从高到低的概率，rk-1为第一个为正确标签，rk-2为前两个中包含标签，rk-5为前五个中包含正确标签。当检测多个人脸时，则取平均值，即mAP。<br><p>[1] (<a href="https://zhuanlan.zhihu.com/p/456060221">https://zhuanlan.zhihu.com/p/456060221</a>)</p><h2 id="Prototype-原型"><a href="#Prototype-原型" class="headerlink" title="Prototype(原型)"></a>Prototype(原型)</h2><div></div>  自然界的物体由各种属性组成，假设一个物体有k个属性x1,x2,...,xk,对应k个系数w1,w2,...,wk,则物体可以简单地表示为一个线性组合 $y=\sum_{j=1}^{k}w_{j}x_{j}$ 。这些属性x可以被成为原型(prototype)。属性的系数w为coefficients。<br><p>[2] (<a href="https://zhuanlan.zhihu.com/p/103512538">https://zhuanlan.zhihu.com/p/103512538</a>)</p><h2 id="Contrastive-learning-对比学习"><a href="#Contrastive-learning-对比学习" class="headerlink" title="Contrastive learning(对比学习)"></a>Contrastive learning(对比学习)</h2><div></div>  对比学习着重于学习同类实例之间的共同特征，区分非同类实例之间的不同之处。与生成式学习比较，对比式学习不需要关注实例上繁琐的细节，只需要在抽象语义级别的特征空间上学会对数据的区分即可，因此模型以及其优化变得更加简单，且泛化能力更强<div align="center">                <img src="https://pic3.zhimg.com/80/v2-3af2ec617b3534ef26336fe9866f402a_720w.jpg"></div><div></div>  对比学习的目标为学习一个编码器，此编码器对同类数据进行相似的编码，并使不同类的数据的编码结果尽可能的不同。<p>[3] (<a href="https://zhuanlan.zhihu.com/p/346686467">https://zhuanlan.zhihu.com/p/346686467</a>)</p><h2 id="Joint-Learning"><a href="#Joint-Learning" class="headerlink" title="Joint Learning"></a>Joint Learning</h2><div></div>  joint Learning是指模型中存在多个子任务，而我们可以将这些子任务一起训练。这么说起来有点抽象，可以举个简单的例子。比如在深度学习中，这个“深度”就可以理解为采用了多个模型进行Joint Learning，比如在许多NLP任务中，在进行模型训练前，我们会使用Word Embedding层（word to vector）对词向量进行编码，而Embedding层的参数，可以在和整个模型一起训练中得到，也可以单独对该层进行预训练，而后在该层参数确定后再拿来和模型剩余部分一起训练，这两种方法都属于Joint Learning的范畴，即Joint Learning中的子任务既可以和整个模型一起训练，也可以单独训练。而Joint Learning的含义则是将多个子模型集成为一个模型，完成最终的目标任务。<h2 id="Instances-实例"><a href="#Instances-实例" class="headerlink" title="Instances(实例)"></a>Instances(实例)</h2><div></div>  实例为目标和语义的结合。即，在图像中将目标检测出来，然后对每个像素打上标签。<h2 id="Landmarks-特征点"><a href="#Landmarks-特征点" class="headerlink" title="Landmarks(特征点)"></a>Landmarks(特征点)</h2><div></div>  图像灰度值发生剧烈变化的点或者在图像边缘上曲率较大的点<h2 id="Siamese-networks"><a href="#Siamese-networks" class="headerlink" title="Siamese networks"></a>Siamese networks</h2><div></div>  连体的神经网络，神经网络的连体通过共享权值来实现。<div align="center">                <img src="https://pic3.zhimg.com/v2-5070e28622a2f3ee9e3cb5d2259fae86_r.jpg"></div><div></div>  左右两个神经网络的权重一模一样。作用为衡量两个输入的相似成都。孪生神经网络有两个输入，将两个输入feed进入两个神经网络，这两个神经网络分别将输入映射到新的空间，形成输入在新的空间中的表示。通过loss的计算，评价两个输入的相似度。<h1 id="Lifelong-Unsupervised-Domain-Adaptive-Person-Re-identification-with-Coordinated-Anti-forgetting-and-Adaptation"><a href="#Lifelong-Unsupervised-Domain-Adaptive-Person-Re-identification-with-Coordinated-Anti-forgetting-and-Adaptation" class="headerlink" title="Lifelong Unsupervised Domain Adaptive Person Re-identification with Coordinated Anti-forgetting and Adaptation"></a>Lifelong Unsupervised Domain Adaptive Person Re-identification with Coordinated Anti-forgetting and Adaptation</h1><p>领域:目标检测 终身学习<br></p><h2 id="Fine-grained-细粒度识别"><a href="#Fine-grained-细粒度识别" class="headerlink" title="Fine-grained(细粒度识别)"></a>Fine-grained(细粒度识别)</h2><div></div>  相比基础的图像识别更加精细化一些。识别具体到小的种类。<h2 id="Anti-forgetting-遗忘问题"><a href="#Anti-forgetting-遗忘问题" class="headerlink" title="Anti-forgetting(遗忘问题)"></a>Anti-forgetting(遗忘问题)</h2><div></div>  灾难性遗忘: 在一个任务上训练出来的模型，如果在一个新任务上进行训练，就会大大降低原任务上的泛化性能，即之前的知识被严重遗忘了。<div></div>  样本遗忘:  在同一个任务的训练过程中，可能会有遗忘现象，一个样本可能在训练过程反复地学了忘，忘了学。<p>[4] (<a href="https://zhuanlan.zhihu.com/p/462224273">https://zhuanlan.zhihu.com/p/462224273</a>)</p><h2 id="Retrieval-based-tasks"><a href="#Retrieval-based-tasks" class="headerlink" title="Retrieval-based tasks"></a>Retrieval-based tasks</h2><div></div>  在外部知识库中搜索所需信息，结合外部知识以及语言模型本身。<h2 id="Source-domain"><a href="#Source-domain" class="headerlink" title="Source domain"></a>Source domain</h2><div></div>  概念来源于迁移学习，在迁移学习中，将已有的知识叫做源域(source domain),要学习的新知识叫做目标域(target domain）。研究如何将源域的知识迁移到目标域(target domain)上。<div></div>  源域（source domain): 与测试样本不同的领域，有丰富的监督信息。<div></div>  目标域(target domain): 测试样本所在的领域，无标签或者只有少量标签。<div align="center">                <img src="https://img-blog.csdnimg.cn/5364ca44799848b4913d8f7d398f79fe.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAbGloZTIwMjE=,size_13,color_FFFFFF,t_70,g_se,x_16"></div><div></div>  红线表示source dataset的颜色信息值分布，蓝线表示target dataset的颜色信息值分布，两个域本来就有shift，导致evaluate模型的准确率降低，则颜色信息不适合选择特征。<div></div>  领域自适应旨在利用各种的feature transformation手段，学习一个域间不变的特征表达（特征自适应）。<p>[5] (<a href="https://blog.csdn.net/lihe4151021/article/details/123763606">https://blog.csdn.net/lihe4151021/article/details/123763606</a>)</p><h2 id="Knowledge-distillation-strategies-知识蒸馏"><a href="#Knowledge-distillation-strategies-知识蒸馏" class="headerlink" title="Knowledge distillation strategies(知识蒸馏)"></a>Knowledge distillation strategies(知识蒸馏)</h2><div></div>  一种模型压缩方法，将训练好的模型包含的知识，蒸馏提取到另一个模型里面去。不同于模型压缩中的剪枝和量化，知识蒸馏是通过构建一个轻量化的小模型，利用性能更好的大模型的监督信息，来训练这个小模型，以期达到更好的性能和精度。<p>功能:</p><ol><li><p>提升模型精度</p></li><li><p>降低模型时延，压缩网络参数</p></li><li><p>图片标签之间的域迁移</p></li><li><p>降低标注量。</p></li></ol><p>[6] (<a href="https://zhuanlan.zhihu.com/p/258390817">https://zhuanlan.zhihu.com/p/258390817</a>)</p><h2 id="Lifelong-Learning-终身学习"><a href="#Lifelong-Learning-终身学习" class="headerlink" title="Lifelong Learning(终身学习)"></a>Lifelong Learning(终身学习)</h2><div></div>  有两个目标：一是应对神经网络由于其自身的设计天然存在的灾难性遗忘问题&nbsp;(McCloskey and Cohen, 1989)，二则是使训练模型更为通用，即令模型同时具备可塑性（学习新知识的能力）和稳定性（对于旧知识的记忆能力）。<p>[7] (<a href="https://zhuanlan.zhihu.com/p/438766442">https://zhuanlan.zhihu.com/p/438766442</a>)</p><h2 id="A-triplet-loss"><a href="#A-triplet-loss" class="headerlink" title="A triplet loss"></a>A triplet loss</h2><div></div>  triplet loss 是深度学习的一种损失函数，主要是用于训练差异性小的样本，比如人脸等；主要功能是表征两个样本之间的相似性，功能同相似性度量。其次在训练目标是得到样本的embedding任务中，triplet loss 也经常使用，比如文本、图片的embedding。<h2 id="Reservoir-sampling-algorithm"><a href="#Reservoir-sampling-algorithm" class="headerlink" title="Reservoir sampling algorithm"></a>Reservoir sampling algorithm</h2><div></div>  蓄水池算法，用于解决大数据流中的数据采样问题。只遍历一次，每次都考虑一个问题：当前元素是否被选中，选中后替换之前选中的哪一个元素。<p>[8] (<a href="https://blog.csdn.net/wq3095435422/article/details/124413184">https://blog.csdn.net/wq3095435422/article/details/124413184</a>)</p><h1 id="Part-based-Pseudo-Label-Refinement-for-Unsupervised-Person-Re-identification"><a href="#Part-based-Pseudo-Label-Refinement-for-Unsupervised-Person-Re-identification" class="headerlink" title="Part-based Pseudo Label Refinement for Unsupervised Person Re-identification"></a>Part-based Pseudo Label Refinement for Unsupervised Person Re-identification</h1><h2 id="pseudo-label-伪标签"><a href="#pseudo-label-伪标签" class="headerlink" title="pseudo-label(伪标签)"></a>pseudo-label(伪标签)</h2><div></div>  来自于半监督学习，半监督学习的核心思想是通过借助无标签的数据来提升有监督过程中的模型性能。伪标签技术就是利用在已标注数据所训练的模型在未标注的数据上进行预测，根据预测结果对样本进行筛选，再次输入模型中进行训练的一个过程。<h2 id="Auxiliary-network"><a href="#Auxiliary-network" class="headerlink" title="Auxiliary network"></a>Auxiliary network</h2><div></div>  第一，辅助任务就像是模拟退火一样，提供了跳出局部最小值的可能。当然只是可能。<div></div>  第二，辅助任务能够优化基于batch的局部误差平面（loss surface）。<div></div>  第三，辅助任务是一种先验，能够更清晰的描述我们的任务。这个在现在众多的启发式网络改进里起了决定性的作用。<p>[9] (<a href="https://www.zhihu.com/question/424682292/answer/1606771422">https://www.zhihu.com/question/424682292/answer/1606771422</a>)</p><h2 id="Teacher-networks-学生-教师网络"><a href="#Teacher-networks-学生-教师网络" class="headerlink" title="Teacher networks:(学生-教师网络)"></a>Teacher networks:(学生-教师网络)</h2><div></div>  主要分为教师网络和学生网络，teacher结构相当于原始复杂的深度神经网络结构，student则是一种轻量级的网络结构；teacher会指导student到达简化参数之后的最好模型效果。在《Distilling the Knowledge in a Neural Network》中，teacher网络对student网络的指导，仅仅在网络输出结果的部分，并且以soft的类别概率分布的方式体现出来。这样student在进行学习的时候，相比于只提供类别的归属信息1或者0来说，它可以知道更多的信息，（虽然两个样本都被划为了1类，只能说明它们被预测为1类的概率大于被预测为0类的概率，但是它们被划分为1类的强弱信息是不知道的）<div align="center">    <img src="https://www.bing.com/images/blob?bcid=r0Qqx9UqUvcEnQ"></div><h2 id="Unsupervised-domain-adaptation"><a href="#Unsupervised-domain-adaptation" class="headerlink" title="Unsupervised domain adaptation"></a>Unsupervised domain adaptation</h2><div></div>  研究的重点为让模型可以适应不同领域之间的差异，减少域差对模型性能的影响。无监督领域自适应一般包含两个领域(数据集),即训练集是带标签的(有监督的)源域(source domain),和无监督的目标域(Target Domain)。任务目标是能够在无重叠视域的目标域中能够检索出同一行人。<h2 id="label-smoothing"><a href="#label-smoothing" class="headerlink" title="label smoothing"></a>label smoothing</h2><div></div>  标签平滑用来解决over-confident的问题，这类问题在对抗构建中尤为重要(GANS)。主要思想:避免模型过拟合，避免output有偏差值时，loss值趋向无穷大而逼迫模型去接近真实的label。<div></div>  在机器学习中，假设样本标签可能存在错误，避免“过分”相信训练样本的标签。当目标函数为交叉熵时，这一思想有非常简单的实现，称为标签平滑(Label smooting),即想办法告诉模型标签不一定正确。<div></div>  理论方式: 在每次迭代时，并不直接将$x_{i}$,$y_{i}$放入训练集，而是设置一个错误率ε，以1-ε的概率将$x_{i}$,$y_{i}$代入训练，以ε的概率将$x_{i}$,$1-y_{i}$代入训练。这样，模型在训练时，既有正确标签输入，又有错误标签输入，可以想象，如此训练出来的模型不会“全力匹配”每一个标签，而只是在一定程度上匹配。这样，如果真的出现错误标签，模型受到的影响就会更小。<h2 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h2><div></div>  目的:预测非球状结构<div></div>   算法原理: 将数据点分为核心点、边界点、噪音点。<div align="center">    <img src="https://www.bing.com/images/blob?bcid=Sx899eKcevcEqxcxoNWLuD9SqbotqVTdPwI"></div><p>[10] (<a href="https://www.jianshu.com/p/e594c2ce0ac0">https://www.jianshu.com/p/e594c2ce0ac0</a>)</p>]]></content>
      
      
      <categories>
          
          <category> 论文理论预读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文预读 </tag>
            
            <tag> 目标检测 </tag>
            
            <tag> 行人重识别 </tag>
            
            <tag> 高一头 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
